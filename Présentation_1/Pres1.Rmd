---
title: |
  | La régression Bêta 
subtitle: "Une alternative intéressante pour modéliser des proportions"
author: Maxime Lacroix
date: 28 septembre 2018
output: 
 beamer_presentation:
    theme: "AnnArbor"
    colortheme: "beaver"
    fonttheme: "structurebold"
---

# Mise en contexte

## Contexte

- Régression sur une variable réponse tenue entre (0,1)
- Par exemple, un taux ou une proportion
- Régression linéaire "classique" à éviter


# Solution : Régression Bêta

## Brève présentation 

- Présentée pour la première fois en 2004 par Ferrari et Cribari-Neto
- Intérêt majeur : 
    - La densité bêta prend différentes formes dépendemment des paramètres
    - Densité généralement hétéroscédastique
    - Interprétation semblable à la régression logistique

## Présentation mathématique

\begin{exampleblock}{Densité d'une loi bêta}
\begin{equation} 
  f_Y\left(y\right) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} y^{\alpha-1}(1-y)^{\beta - 1} \quad y \in (0,1) \quad \alpha,\beta > 0
  \label{eq:beta_base}
\end{equation} 
\end{exampleblock}

De l'équation \ref{eq:beta_base}, Ferrari et Cribari-Neto ont proposé une nouvelle paramétrisation, en posant :
\begin{itemize}
  \item $\mu = \frac{\alpha}{\alpha + \beta}$
  \item $\phi = \alpha + \beta$
\end{itemize}  

## Nouvelle paramétrisation

\begin{exampleblock}{Densité sous la nouvelle paramétrisation}
\begin{equation} 
  f_{Y}\left(y\right) = \frac{\Gamma(\mu\phi)}{\Gamma((1-\mu)\phi)} y^{\mu\phi-1}(1-y)^{(1-\mu)(\phi-1)}
  \label{eq:beta_nouvelle}
\end{equation} 
\end{exampleblock}

On peut donc dire que $Y\sim B(\mu,\phi)$. L'équation \ref{eq:beta_nouvelle} nous donne les propriétés suivantes : 
\begin{itemize}
  \item $E(Y) = \mu$
  \item $Var(Y) = \frac{\mu(1-\mu)}{1+\phi}$
\end{itemize} 

On appelle d'ailleurs $\phi$ le paramètre de dispersion.

# Modèle de régression

## Définition du modèle
On peut maintenant définir le modèle pratiquement comme un GLM, c'est à dire : 
\begin{exampleblock}{Modèle de régression bêta simple}
\begin{equation} 
  g(\mu_i) = x_i^t\beta = \eta_i
  \label{eq:beta_mod}
\end{equation} 
\end{exampleblock}

La fonction de lien $g()$ peut être choisie comme pour un GLM classique, soit en utilisant le logit, le log-log, le Cauchy. C'est au choix de l'utilisateur. De base, le package ```betareg``` utilise le lien logit. 

La variance d'une observation $y_i$ est donnée par la formule suivante. On remarque facilement qu'elle dépend de $\mu_i$, donc il y a hétéroscédasticité.

\begin{exampleblock}{}
\begin{equation} 
  VAR(y_i) = \frac{\mu_i(1- \mu_i)}{1 + \phi}
  \label{eq:var}
\end{equation} 
\end{exampleblock}

## Paramètre de dispersion non-constant

Smithson et Verkuilen, en 2006, ont proposé un modèle où le paramètre de dispersion est non-constant. On se retrouve donc avec un autre paramètre à estimer. La régression est donnée par : 

\begin{exampleblock}{Modèle de régression bêta avec dispersion changeante}
\begin{equation} 
  g_1(\mu_i) = x_i^t\beta = \eta_{1i}
  \label{eq:var}
\end{equation}
\begin{equation} 
  g_2(\phi_i) = z_i^t\gamma = \eta_{2i}
  \label{eq:var4}
\end{equation}
\end{exampleblock}

## Estimation des paramètres

Les paramètres sont estimés en maximisant la vraisemblance. La fonction log-vraisemblance est aisémant calculable, elle est donnée par : 
\begin{exampleblock}{Fonction du log-vraisemblance}
\begin{equation}
\begin{aligned}
l_i(\mu_i,\phi_i) =  & log\Gamma(\phi_i) - log\Gamma(\mu_i\phi_i) - \\
      & log\Gamma((1-\mu_i)\phi_i) + (\mu_i\phi_i - 1)log(y_i) + \\
      & ((1-\mu_i)\phi_i-1)log(1-y_i)
      \label{eq:var}
\end{aligned}
\end{equation}
\end{exampleblock}

# Exemple d'utilisation

## Jeu de données utilisé

- Titre : Proportion of seats held by women in national parliaments (%)
- Source : https://datahub.io/world-bank/sg.gen.parl.zs#resource-data
- Nombre de données : 3917 
- Variables : 
    - Nom du pays
    - Code ISO du pays
    - Continent*
    - Année
    - Décennie*
    - Proportion

---

```{r echo=FALSE}
knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})
```

```{r, fig.cap="", echo=FALSE,warning=FALSE,message=FALSE}
source("code_base.R")
```

## Implémentation en R

Supposons que l'on veut prédire la proportion de femmes en fonction du continent. Comme il s'agit d'une proportion, on peut utiliser la régression bêta. Le package ```betareg``` nous permet d'effectuer cette analyse. L'écriture à utiliser est la suivante : 

```{r, fig.cap="", echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
betareg(formula, data, subset, na.action, weights, offset,
  link = c("logit", "probit", "cloglog", 
           "cauchit", "log", "loglog"),
  link.phi = NULL, type = c("ML", "BC", "BR"),
  control = betareg.control(...), model = TRUE,
  y = TRUE, x = FALSE, ...)
```

## Premier modèle : Lien logit et dispersion fixe 

```{r, fig.cap="", echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
library(betareg)
mod1 <- betareg(Value~continent,
                link = "logit",
                data=dat_prop)
summary(mod1)
```

## Résumé du modèle


```{r, fig.cap="", echo=FALSE,warning=FALSE,message=FALSE,eval=TRUE,mysize=TRUE, size='\\tiny'}
library(betareg)
library(pander)
mod1 <- betareg(Value~continent,link = ("logit"),data=dat_prop)
summary(mod1)
```

## Deuxième modèle : Lien logit et dispersion changeante

```{r, fig.cap="", echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
mod2 <- betareg(Value~continent|continent,
                link = "logit",
                link.phi = "log",
                data=dat_prop)
summary(mod2)
```

## Résumé du modèle


```{r, fig.cap="", echo=FALSE,warning=FALSE,message=FALSE,eval=TRUE,mysize=TRUE, size='\\tiny'}
mod2 <- betareg(Value~continent|continent,
                link = "logit",
                link.phi = "log",
                data=dat_prop)
summary(mod2)
```

## Troisième modèle : Lien log et dispersion constante

```{r, fig.cap="", echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
mod3 <- betareg(Value~continent|1,
                link = "log",
                data=dat_prop)
summary(mod3)
```

## Résumé du modèle


```{r, fig.cap="", echo=FALSE,warning=FALSE,message=FALSE,eval=TRUE, mysize=TRUE, size='\\tiny'}
mod3 <- betareg(Value~continent|1,
                link = "log",
                data=dat_prop)
summary(mod3)
```

## Comparaison des différents modèles 

Les différents modèles nous ont montré différentes choses :

- Peu importe le lien choisi, l'estimation de $\phi$ est la même si on choisi que $\phi$ est constant.
- Comme on maximise la vraisemblance, il est naturel que les coefficients pour $\mu$ ne soient pas les mêmes pour les modèles 1 et 2.

Quel est le meilleur modèle?

## Test du maximum de vraisemblance


On pourrait s'intéresser à connaître l'impact d'ajouter les coefficients de dispersion. En posant : 

- $H_0$ = Modèle simple 
- $H_1$ = Modèle complexe 

On peut utiliser le code suivant : 


```{r, fig.cap="", echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
library(lmtest)
lrtest(mod1,mod2)
```


## Résultat du test 

```{r, fig.cap="", echo=FALSE,warning=FALSE,message=FALSE,eval=TRUE}
library(lmtest)
lrtest(mod1,mod2)
```


## Choix selon un critère

Tout comme dans les GLM, il est possible de choisir le meilleur en utilisant le critère de notre choix, comme l'AIC par exemple, ou le BIC. 
Il se peut que le choix du type de lien $g()$ améliore grandement le modèle, dépendemment des données. 
Le code pour effectuer ce calcul est le suivant : 
```{r, fig.cap="", echo=TRUE,warning=FALSE,message=FALSE,eval=TRUE}
AIC(mod1,mod2,mod3)
```

# Méthodes avancées

## Contexte

En 2010, Grün, Kosmidis et Zeilis ont publié l'article *Extended Beta Regression in R: Shaken, Stirred, Mixed and Partitioned*, dans lequel ils expliquent différentes méthodes plus complexes pouvant améliorer le modèle. Trois gros thèmes y ressortent, soient : 

- Correction de biais dans l'estimation des paramètres
- Les arbres de régression bêta
- Mixtures ou mélanges de régressions bêta

## Correction de biais

Dans l'article, on discute le fait que la méthode du maximum de vraisemblance pour estimer les paramètres de $\phi$ a tendance à sous-estimer l'écart-type des paramètres, ce qui amène des problèmes notamment au niveau de l'inférence que l'on peut faire avec le modèle. 

Solution : 

- Correction du biais (BC)
- Réduction du biais (BR)

## Implémentation en R

Rappel : Écriture de la fonction ```betareg``` : 

```{r, fig.cap="", echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
betareg(formula, data, subset, na.action, weights, offset,
  link = c("logit", "probit", "cloglog", 
           "cauchit", "log", "loglog"),
  link.phi = NULL, type = c("ML", "BC", "BR"),
  control = betareg.control(...), model = TRUE,
  y = TRUE, x = FALSE, ...)
```

Remarquons le ```type``` correspond à la méthode d'estimation choisie.


## Arbres de régression bêta

L'idée derrière cette méthode est de se questionner à savoir s'il n'y aurait pas une autre variable qui permettrait de partitionner le modèle. Après avoir partitionné le modèle, on peut estimer les paramètres dans chacune des partitions. Voici un example simple : 

```{r, fig.cap="", echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
mod4 <- betatree(Value~continent,
                 ~Decennie, 
                 link = "logit",
                 data=dat_prop,
                 minsize=100)
plot(mod4)
```


## Résultat 

```{r, fig.cap="", echo=FALSE,warning=FALSE,message=FALSE,eval=TRUE}
mod4 <- betatree(Value~continent,
                 ~Decennie, 
                 link = "logit",
                 data=dat_prop,
                 minsize=100)
plot(mod4)
```

On voit que de faire un modèle différent par décennie semble une option intéressante. 

## Mélange de régressions bêta

On pourrait supposer qu'il y a des différences dans différents sous-groupes de l'échantillon, mais qu'il n'existe pas de variables discriminantes. La solution : utiliser ```betamix```. Voici un example. Ça revient à créer 3 _clusters_ de données et d'estimer un modèle pour chacun des _clusters_.

```{r, fig.cap="", echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
rs_mix <- betamix(accuracy ~ iq, 
                  data = ReadingSkills, k = 3,
                  nstart = 10, 
                  extra_components = extraComponent(
                  type = "uniform",
                  oef = 0.99, delta = 0.01))
```



# Conclusion

## Conclusion

- Modèle relativement simple à utiliser
- Possibilité de complexifier les choses rapidement
- Règle beaucoup de problèmes avec la dispersion des données que la régression linéaire n'est pas en mesure de faire.
